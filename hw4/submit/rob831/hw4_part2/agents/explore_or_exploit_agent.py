from collections import OrderedDict

from rob831.hw4_part2.critics.dqn_critic import DQNCritic
from rob831.hw4_part2.critics.cql_critic import CQLCritic
from rob831.hw4_part2.infrastructure.replay_buffer import ReplayBuffer
from rob831.hw4_part2.infrastructure.utils import *
from rob831.hw4_part2.policies.argmax_policy import ArgMaxPolicy
from rob831.hw4_part2.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer
from rob831.hw4_part2.exploration.rnd_model import RNDModel
from .dqn_agent import DQNAgent
import numpy as np


class ExplorationOrExploitationAgent(DQNAgent):
    def __init__(self, env, agent_params, normalize_rnd=True, rnd_gamma=0.99):
        super(ExplorationOrExploitationAgent, self).__init__(env, agent_params)

        # 1) Create the buffer FIRST so we can query its processed obs shape
        self.replay_buffer = MemoryOptimizedReplayBuffer(100000, 1, float_obs=True)

        # The super().__init__ set self.last_obs via env.reset(); we can now encode once
        # to learn what the networks will actually receive (e.g., 16x16 -> 256).
        # Temporarily store the current frame so encode_recent_observation has data.
        tmp_idx = self.replay_buffer.store_frame(self.last_obs)
        processed = self.replay_buffer.encode_recent_observation()   # e.g., shape (16,16)
        true_ob_dim = int(np.prod(processed.shape))
        # (optional) drop the temporary effect slot (harmless to leave as well)
        # NOTE: leaving it is fine; training will quickly overwrite.

        # 2) Clone agent_params and overwrite ob_dim with the processed size
        _ap = dict(agent_params)
        _ap['ob_dim'] = true_ob_dim

        self.num_exploration_steps = _ap['num_exploration_steps']
        self.offline_exploitation   = _ap['offline_exploitation']

        # 3) Build critics/RND **after** we know the correct ob_dim
        self.exploitation_critic = DQNCritic(_ap, self.optimizer_spec)
        self.exploration_critic  = DQNCritic(_ap, self.optimizer_spec)
        self.exploration_model   = RNDModel(_ap, self.optimizer_spec)

        self.explore_weight_schedule = _ap['explore_weight_schedule']
        self.exploit_weight_schedule = _ap['exploit_weight_schedule']

        self.actor       = ArgMaxPolicy(self.exploration_critic)
        self.eval_policy = ArgMaxPolicy(self.exploitation_critic)

        self.exploit_rew_shift = _ap['exploit_rew_shift']
        self.exploit_rew_scale = _ap['exploit_rew_scale']
        self.eps               = _ap['eps']

        self.running_rnd_rew_std = 1.0
        self.normalize_rnd = normalize_rnd
        self.rnd_gamma = rnd_gamma

    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):
        log = {}

        # After exploration is over, act according to exploitation critic
        if self.t > self.num_exploration_steps:
            self.actor.set_critic(self.exploitation_critic)

        if (self.t > self.learning_starts
                and self.t % self.learning_freq == 0
                and self.replay_buffer.can_sample(self.batch_size)):

            # Reward weights from schedules (fallbacks if schedules missing)
            try:
                explore_weight = self.explore_weight_schedule.value(self.t)
            except Exception:
                explore_weight = 1.0
            try:
                exploit_weight = self.exploit_weight_schedule.value(self.t)
            except Exception:
                exploit_weight = 0.0

            # --- Exploration bonus on current states s ---
            expl_bonus = self.exploration_model.forward_np(ob_no)  # [B]

            # Normalize the bonus with EMA of std
            if self.normalize_rnd:
                batch_std = float(np.std(expl_bonus) + 1e-8)
                self.running_rnd_rew_std = (
                    self.rnd_gamma * self.running_rnd_rew_std
                    + (1.0 - self.rnd_gamma) * max(batch_std, 1e-8)
                )
                expl_bonus = expl_bonus / (self.running_rnd_rew_std + 1e-8)

            # Mixed reward for exploration critic
            mixed_reward = explore_weight * expl_bonus + exploit_weight * re_n

            # Environment reward for exploitation critic (shift/scale even in part 1 is fine)
            env_reward = (re_n + self.exploit_rew_shift) * self.exploit_rew_scale

            # --- Updates ---
            # 1) RND model on next states s'
            expl_model_loss = self.exploration_model.update(next_ob_no)

            exploration_critic_loss = self.exploration_critic.update(
                ob_no, ac_na, next_ob_no, mixed_reward, terminal_n
            )

            # 3) Update exploitation critic on env reward
            exploitation_critic_loss = self.exploitation_critic.update(
                ob_no, ac_na, next_ob_no, env_reward, terminal_n
            )

            # Target networks
            if self.num_param_updates % self.target_update_freq == 0:
                self.exploration_critic.update_target_network()
                self.exploitation_critic.update_target_network()

            # Logging
            log['Exploitation Critic Loss'] = exploitation_critic_loss['Training Loss']
            log['Exploration Critic Loss']  = exploration_critic_loss['Training Loss']
            log['Exploration Model Loss']   = expl_model_loss

            self.num_param_updates += 1

        self.t += 1
        return log

    def step_env(self):
        """
        Unchanged logic; the buffer stores raw obs, and critics/RND now know the correct ob_dim.
        """
        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs)

        perform_random_action = np.random.random() < self.eps or self.t < self.learning_starts

        if perform_random_action:
            action = self.env.action_space.sample()
        else:
            processed = self.replay_buffer.encode_recent_observation()
            action = self.actor.get_action(processed)

        next_obs, reward, done, info = self.env.step(action)
        self.last_obs = next_obs.copy()

        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.env.reset()
